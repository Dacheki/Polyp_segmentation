{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import jaccard_score, recall_score, precision_score, f1_score\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import random\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import pandas as pd\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logger = logging.getLogger(\"test_logger\")\n",
    "test_logger.setLevel(logging.INFO)\n",
    "file_handler = logging.FileHandler(\"test_results.log\", mode=\"w\")\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "file_handler.setFormatter(formatter)\n",
    "test_logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 20\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_train_augmentations():\n",
    "#     return A.Compose([\n",
    "#         A.Resize(256, 256),\n",
    "#         # Geometric transformations\n",
    "#         A.HorizontalFlip(p=0.5),\n",
    "#         A.VerticalFlip(p=0.5),\n",
    "#         A.Rotate(\n",
    "#                 limit=359,\n",
    "#                 border_mode=cv2.BORDER_CONSTANT,\n",
    "#                 value=0,          \n",
    "#                 mask_value=0,     \n",
    "#                 p=0.5\n",
    "#             ),\n",
    "#         # Noise and blur\n",
    "#         A.GaussNoise(\n",
    "#                 var_limit=(0.0005, 0.003),  \n",
    "#                 mean=0.0,                   \n",
    "#                 p=0.5\n",
    "#             ),\n",
    "#         A.MultiplicativeNoise(multiplier=(0.9, 1.1), p=0.2),\n",
    "#         A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
    "#         A.MotionBlur(blur_limit=7, p=0.2),\n",
    "#         # Occlusion and artifacts\n",
    "#         A.CoarseDropout(max_holes=8, max_height=32, max_width=32, \n",
    "#                        min_holes=1, min_height=8, min_width=8, \n",
    "#                        fill_value=0, p=0.3),\n",
    "#         # Normalization\n",
    "#         A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#         ToTensorV2(),\n",
    "#     ], additional_targets={'mask': 'mask'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PolypDataset(Dataset):\n",
    "#     def __init__(self, image_dir, mask_dir, transform=None):\n",
    "#         self.image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.*\")))\n",
    "#         self.mask_paths = sorted(glob.glob(os.path.join(mask_dir, \"*.*\")))\n",
    "#         assert len(self.image_paths) == len(self.mask_paths), \\\n",
    "#             f\"Number of images ({len(self.image_paths)}) and masks ({len(self.mask_paths)}) don't match!\"\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_paths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image = cv2.cvtColor(cv2.imread(self.image_paths[idx]), cv2.COLOR_BGR2RGB)\n",
    "#         mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "#         if self.transform:\n",
    "#             transformed = self.transform(image=image, mask=mask)\n",
    "#             image = transformed[\"image\"]\n",
    "#             mask = transformed[\"mask\"]\n",
    "#             # Ensure mask is float32 and in [0,1] range\n",
    "#             mask = mask.float() / 255.0\n",
    "#         else:\n",
    "#             image = cv2.resize(image, (256, 256)) / 255.0\n",
    "#             mask = (cv2.resize(mask, (256, 256)) > 127).astype(np.float32)  # Changed to float32\n",
    "#             image = (image - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "#             image = torch.from_numpy(image.transpose(2, 0, 1)).float()\n",
    "#             mask = torch.from_numpy(mask).unsqueeze(0).float()  # Ensure float\n",
    "            \n",
    "#         filename = os.path.basename(self.image_paths[idx])\n",
    "#         return image, mask, filename\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    # Convert to numpy arrays if they're tensors\n",
    "    if isinstance(y_true, torch.Tensor):\n",
    "        y_true = y_true.cpu().numpy()\n",
    "    if isinstance(y_pred, torch.Tensor):\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    # Flatten and threshold predictions to binary (0 or 1)\n",
    "    y_true = y_true.flatten().astype(int)  # Ensure it's integer type\n",
    "    y_pred = (y_pred.flatten() > 0.5).astype(int)  # Threshold at 0.5\n",
    "    \n",
    "    # Calculate metrics\n",
    "    iou = jaccard_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    return iou, rec, prec, f1\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCELoss()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # Ensure target is float (in case it's not)\n",
    "        target = target.float()\n",
    "        \n",
    "        smooth = 1.0\n",
    "        pred = pred.contiguous().view(-1)\n",
    "        target = target.contiguous().view(-1)\n",
    "        \n",
    "        # BCE Loss\n",
    "        bce_loss = self.bce(pred, target)\n",
    "        \n",
    "        # Dice Loss\n",
    "        intersection = (pred * target).sum()\n",
    "        dice_coeff = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "        dice_loss = 1 - dice_coeff\n",
    "        \n",
    "        return dice_loss + bce_loss\n",
    "\n",
    "# def train_one_epoch(model, dataloader, criterion, optimizer, device, writer, epoch, dataset_name):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     all_y_true = []\n",
    "#     all_y_pred = []\n",
    "\n",
    "#     for images, masks, _ in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "#         images = images.to(device)\n",
    "#         masks = masks.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, masks)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item() * images.size(0)\n",
    "\n",
    "#         preds = (outputs > 0.5).float().detach()\n",
    "#         all_y_true.append(masks.cpu().numpy())\n",
    "#         all_y_pred.append(preds.cpu().numpy())\n",
    "\n",
    "#     epoch_loss = running_loss / len(dataloader.dataset)\n",
    "#     all_y_true = np.concatenate(all_y_true, axis=0)\n",
    "#     all_y_pred = np.concatenate(all_y_pred, axis=0)\n",
    "\n",
    "#     iou, recall, precision, f1 = metrics(all_y_true, all_y_pred)\n",
    "\n",
    "#     writer.add_scalar(f\"Loss/Train/{dataset_name}\", epoch_loss, epoch)\n",
    "#     writer.add_scalar(f\"Metrics/Train_IoU/{dataset_name}\", iou, epoch)\n",
    "#     writer.add_scalar(f\"Metrics/Train_Recall/{dataset_name}\", recall, epoch)\n",
    "#     writer.add_scalar(f\"Metrics/Train_Precision/{dataset_name}\", precision, epoch)\n",
    "#     writer.add_scalar(f\"Metrics/Train_F1/{dataset_name}\", f1, epoch)\n",
    "#     torch.cuda.empty_cache()\n",
    "#     return epoch_loss, iou, recall, precision, f1\n",
    "\n",
    "def validate(model, dataloader, criterion, device, writer, epoch, dataset_name):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks, _ in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            preds = (outputs > 0.5).float().detach()\n",
    "            all_y_true.append(masks.cpu().numpy())\n",
    "            all_y_pred.append(preds.cpu().numpy())\n",
    "\n",
    "    val_loss = running_loss / len(dataloader.dataset)\n",
    "    all_y_true = np.concatenate(all_y_true, axis=0)\n",
    "    all_y_pred = np.concatenate(all_y_pred, axis=0)\n",
    "    iou, recall, precision, f1 = metrics(all_y_true, all_y_pred)\n",
    "\n",
    "    writer.add_scalar(f\"Loss/Val/{dataset_name}\", val_loss, epoch)\n",
    "    writer.add_scalar(f\"Metrics/Val_IoU/{dataset_name}\", iou, epoch)\n",
    "    writer.add_scalar(f\"Metrics/Val_Recall/{dataset_name}\", recall, epoch)\n",
    "    writer.add_scalar(f\"Metrics/Val_Precision/{dataset_name}\", precision, epoch)\n",
    "    writer.add_scalar(f\"Metrics/Val_F1/{dataset_name}\", f1, epoch)\n",
    "\n",
    "    evaluate_and_save(model, dataloader, criterion, device, save_csv_path=f\"results/val_predictions.csv\", threshold=0.5)\n",
    "\n",
    "    return val_loss, iou, recall, precision, f1\n",
    "\n",
    "def test_model(model, dataloader, criterion, device, save_folder=\"predicted_masks\"):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks, fnames in tqdm(dataloader, desc=\"Testing\", leave=False):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, masks)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            preds = (outputs > 0.5).float().detach()\n",
    "            all_y_true.append(masks.cpu().numpy())\n",
    "            all_y_pred.append(preds.cpu().numpy())\n",
    "\n",
    "            preds_np = preds.cpu().numpy()\n",
    "            for i in range(preds_np.shape[0]):\n",
    "                pred_mask = (preds_np[i, 0] * 255).astype(np.uint8)\n",
    "                save_path = os.path.join(save_folder, fnames[i])\n",
    "                cv2.imwrite(save_path, pred_mask)\n",
    "\n",
    "    test_loss = running_loss / len(dataloader.dataset)\n",
    "    all_y_true = np.concatenate(all_y_true, axis=0)\n",
    "    all_y_pred = np.concatenate(all_y_pred, axis=0)\n",
    "\n",
    "    iou, recall, precision, f1 = metrics(all_y_true, all_y_pred)\n",
    "\n",
    "    evaluate_and_save(model, dataloader, criterion, device, save_csv_path=f\"results/test_predictions.csv\", threshold=0.5)\n",
    "\n",
    "    return test_loss, iou, recall, precision, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = {\"name\": \"40_60\", \n",
    "#      \"train_image_dir\": 'C:/Users/Dara/Desktop/Final_thesis/data_choosing/Images/train/40_60',\n",
    "#      \"train_mask_dir\": \"C:/Users/Dara/Desktop/Final_thesis/data_choosing/Masks/train/40_60\",\n",
    "#      \"val_image_dir\": \"C:/Users/Dara/Desktop/Final_thesis/data_choosing/Images/val\",\n",
    "#      \"val_mask_dir\": \"C:/Users/Dara/Desktop/Final_thesis/data_choosing/Masks/val\",\n",
    "#      \"test_image_dir\": \"C:/Users/Dara/Desktop/Final_thesis/data_choosing/Images/test\",\n",
    "#      \"test_mask_dir\": \"C:/Users/Dara/Desktop/Final_thesis//data_choosing/Masks/test\"}\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# results = []\n",
    "# encoder_name = \"resnet34\"  \n",
    "\n",
    "\n",
    "# dataset_name = dataset[\"name\"]\n",
    "# print(f\"Training on dataset: {dataset_name}\")\n",
    "\n",
    "# train_dataset = PolypDataset(dataset[\"train_image_dir\"], dataset[\"train_mask_dir\"], transform=get_train_augmentations())\n",
    "# val_dataset = PolypDataset(dataset[\"val_image_dir\"], dataset[\"val_mask_dir\"])\n",
    "# test_dataset = PolypDataset(dataset[\"test_image_dir\"], dataset[\"test_mask_dir\"])\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# model = smp.UnetPlusPlus(\n",
    "#     encoder_name=encoder_name,\n",
    "#     encoder_weights=\"imagenet\",\n",
    "#     in_channels=3,\n",
    "#     classes=1,\n",
    "#     activation=\"sigmoid\"\n",
    "# )\n",
    "# model = model.to(device)\n",
    "\n",
    "# criterion = DiceBCELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# log_dir = f\"logs/{dataset_name}_{encoder_name}\"\n",
    "# writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# patience = 10\n",
    "# best_val_iou = 0.0\n",
    "# best_epoch = -1\n",
    "# epochs_without_improvement = 0\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f\"Эпоха {epoch+1}/{num_epochs} для датасета {dataset_name}\")\n",
    "    \n",
    "#     train_loss, train_iou, train_recall, train_precision, train_f1 = train_one_epoch(\n",
    "#         model, train_loader, criterion, optimizer, device, writer, epoch, dataset_name)\n",
    "#     print(f\"Train Loss: {train_loss:.4f} | IoU: {train_iou:.4f} | Recall: {train_recall:.4f} | Precision: {train_precision:.4f} | F1: {train_f1:.4f}\")\n",
    "    \n",
    "#     val_loss, val_iou, val_recall, val_precision, val_f1 = validate(\n",
    "#         model, val_loader, criterion, device, writer, epoch, dataset_name)\n",
    "#     print(f\"Val Loss: {val_loss:.4f} | IoU: {val_iou:.4f} | Recall: {val_recall:.4f} | Precision: {val_precision:.4f} | F1: {val_f1:.4f}\")\n",
    "\n",
    "#     if val_iou > best_val_iou:\n",
    "#         best_val_iou = val_iou\n",
    "#         best_epoch = epoch\n",
    "#         torch.save(model.state_dict(), f\"unetplusplus_{dataset_name}_best_model.pth\")\n",
    "#         print(\"Новая лучшая модель сохранена.\")\n",
    "#         epochs_without_improvement = 0\n",
    "#     else:\n",
    "#         epochs_without_improvement += 1\n",
    "#         if epochs_without_improvement >= patience:\n",
    "#             print(f\"Ранняя остановка на эпохе {epoch+1}\")\n",
    "#             break\n",
    "\n",
    "# print(f\"Обучение завершено для датасета {dataset_name}. Лучшая модель на эпохе {best_epoch+1} с IoU: {best_val_iou:.4f}\")\n",
    "\n",
    "# model.load_state_dict(torch.load(f\"unetplusplus_{dataset_name}_best_model.pth\"))\n",
    "# test_loss, test_iou, test_recall, test_precision, test_f1 = test_model(\n",
    "#     model, test_loader, criterion, device, save_folder=f\"predicted_masks_{dataset_name}\")\n",
    "\n",
    "# results.append({\n",
    "#     \"Dataset\": dataset_name,\n",
    "#     \"Best Epoch\": best_epoch + 1,\n",
    "#     \"Test IoU\": test_iou,\n",
    "#     \"Test Recall\": test_recall,\n",
    "#     \"Test Precision\": test_precision,\n",
    "#     \"Test F1\": test_f1,\n",
    "#     \"Test Loss\": test_loss\n",
    "# })\n",
    "\n",
    "# writer.close()\n",
    "\n",
    "# results_df = pd.DataFrame(results)\n",
    "# print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_model_with_ground_truth(model, dataloader, criterion, device, save_folder=\".\", threshold=0.5):\n",
    "#     model.eval()\n",
    "#     running_loss = 0.0\n",
    "#     all_pred_data = []\n",
    "    \n",
    "#     if not os.path.exists(save_folder):\n",
    "#         os.makedirs(save_folder)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, masks, filenames in tqdm(dataloader, desc=\"Testing\"):\n",
    "#             images = images.to(device)\n",
    "#             masks = masks.to(device)\n",
    "\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, masks)\n",
    "#             running_loss += loss.item() * images.size(0)\n",
    "\n",
    "#             preds = outputs.float()\n",
    "#             preds_np = preds.cpu().numpy()\n",
    "#             masks_np = masks.cpu().numpy()  # Ground truth masks\n",
    "            \n",
    "#             for i in range(preds_np.shape[0]):\n",
    "#                 mask_pred = preds_np[i, 0]\n",
    "#                 mask_true = masks_np[i, 0]  # Ground truth mask\n",
    "                \n",
    "#                 # Save predicted masks\n",
    "#                 mask_img = ((mask_pred > threshold) * 255).astype(np.uint8)\n",
    "#                 cv2.imwrite(os.path.join(save_folder, filenames[i]), mask_img)\n",
    "                \n",
    "#                 # Collect all data points (not sampled)\n",
    "#                 h, w = mask_pred.shape\n",
    "#                 for y in range(h):\n",
    "#                     for x in range(w):\n",
    "#                         # Only save points where true value is 1 (or all if you prefer)\n",
    "#                         # To save all points, remove the if condition\n",
    "#                         if mask_true[y, x] == 1:  # This saves only True values\n",
    "#                             all_pred_data.append({\n",
    "#                                 'filename': filenames[i],\n",
    "#                                 'x': x,\n",
    "#                                 'y': y,\n",
    "#                                 'pred_value': float(mask_pred[y, x]),\n",
    "#                                 'true_value': int(mask_true[y, x]),  # 0 or 1\n",
    "#                                 'above_threshold': int(mask_pred[y, x] > threshold)\n",
    "#                             })\n",
    "\n",
    "#     # Save all data\n",
    "#     preds_df = pd.DataFrame(all_pred_data)\n",
    "#     preds_csv_path = os.path.join(save_folder, 'predictions_with_ground_truth.csv')\n",
    "#     preds_df.to_csv(preds_csv_path, index=False)\n",
    "#     print(f\"Data with predictions and ground truth saved to {preds_csv_path}\")\n",
    "\n",
    "#     # Calculate metrics\n",
    "#     test_loss = running_loss / len(dataloader.dataset)\n",
    "    \n",
    "#     # Calculate metrics for different thresholds\n",
    "#     thresholds = 0.5\n",
    "#     metrics_df = pd.DataFrame(columns=['threshold', 'precision', 'recall', 'f1'])\n",
    "    \n",
    "#     for thresh in thresholds:\n",
    "#         preds_df['binary_pred'] = (preds_df['pred_value'] > thresh).astype(int)\n",
    "#         precision = precision_score(preds_df['true_value'], preds_df['binary_pred'], zero_division=0)\n",
    "#         recall = recall_score(preds_df['true_value'], preds_df['binary_pred'], zero_division=0)\n",
    "#         f1 = f1_score(preds_df['true_value'], preds_df['binary_pred'], zero_division=0)\n",
    "        \n",
    "#         metrics_df = pd.concat([metrics_df, pd.DataFrame({\n",
    "#             'threshold': [thresh],\n",
    "#             'precision': [precision],\n",
    "#             'recall': [recall],\n",
    "#             'f1': [f1]\n",
    "#         })], ignore_index=True)\n",
    "    \n",
    "#     metrics_csv_path = os.path.join(save_folder, 'metrics_by_threshold.csv')\n",
    "#     metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "#     print(f\"Metrics for different thresholds saved to {metrics_csv_path}\")\n",
    "\n",
    "#     return test_loss, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []\n",
    "# encoder_name = \"resnet34\"  \n",
    "\n",
    "# dataset_name = dataset[\"name\"]\n",
    "# print(f\"Training on dataset: {dataset_name}\")\n",
    "\n",
    "# train_dataset = PolypDataset(dataset[\"train_image_dir\"], dataset[\"train_mask_dir\"], transform=get_train_augmentations())\n",
    "# val_dataset = PolypDataset(dataset[\"val_image_dir\"], dataset[\"val_mask_dir\"])\n",
    "# test_dataset = PolypDataset(dataset[\"test_image_dir\"], dataset[\"test_mask_dir\"])\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# model = smp.UnetPlusPlus(\n",
    "#     encoder_name=encoder_name,\n",
    "#     encoder_weights=\"imagenet\",\n",
    "#     in_channels=3,\n",
    "#     classes=1,\n",
    "#     activation=\"sigmoid\"\n",
    "# )\n",
    "# model = model.to(device)\n",
    "\n",
    "# criterion = DiceBCELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# log_dir = f\"logs/{dataset_name}_{encoder_name}\"\n",
    "# writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# patience = 10\n",
    "# best_val_iou = 0.0\n",
    "# best_epoch = -1\n",
    "# epochs_without_improvement = 0\n",
    "\n",
    "# # Load the best model\n",
    "# model.load_state_dict(torch.load(f\"unetplusplus_{dataset_name}_best_model.pth\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on dataset: 40_60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_12224\\3181544907.py:149: UserWarning: Argument 'fill_value' is not valid and will be ignored.\n",
      "  A.CoarseDropout(\n",
      "Evaluating: 100%|██████████| 193/193 [00:26<00:00,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved evaluation results to results/test_predictions.csv\n",
      "  Dataset  Best Epoch  Test IoU  Test Recall  Test Precision   Test F1  \\\n",
      "0   40_60           0  0.774491     0.857483        0.888915  0.872916   \n",
      "\n",
      "   Test Loss  \n",
      "0   0.527112  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import jaccard_score, recall_score, precision_score, f1_score\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import random\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import pandas as pd\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs(\"visualizations\", exist_ok=True)\n",
    "\n",
    "def visualize_samples_with_names(original_images, original_masks, augmented_images, augmented_masks, filenames, epoch=0, prefix=\"train\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    def denormalize(tensor):\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406], device=tensor.device).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225], device=tensor.device).view(3, 1, 1)\n",
    "        return (tensor * std + mean).clamp(0, 1)\n",
    "\n",
    "    def to_uint8(tensor_img):\n",
    "        img = tensor_img.permute(1, 2, 0).cpu().numpy()\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "        return img\n",
    "\n",
    "    num_samples = min(5, len(original_images))\n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(18, num_samples * 4))\n",
    "\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Original Image\n",
    "        orig_img = (original_images[i].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "        axes[i, 0].imshow(orig_img)\n",
    "        axes[i, 0].set_title(f\"Original Image\\n{filenames[i]}\")\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        orig_mask = original_masks[i].cpu().numpy()\n",
    "        if orig_mask.ndim == 3:\n",
    "            orig_mask = orig_mask[0]\n",
    "        axes[i, 1].imshow(orig_mask, cmap='gray')\n",
    "        axes[i, 1].set_title(\"Original Mask\")\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "        aug_img = to_uint8(denormalize(augmented_images[i]))\n",
    "        axes[i, 2].imshow(aug_img)\n",
    "        axes[i, 2].set_title(f\"Augmented Image\\n{filenames[i]}\")\n",
    "        axes[i, 2].axis('off')\n",
    "\n",
    "        aug_mask = augmented_masks[i].cpu().numpy()\n",
    "        if aug_mask.ndim == 3:\n",
    "            aug_mask = aug_mask[0]\n",
    "        axes[i, 3].imshow(aug_mask, cmap='gray')\n",
    "        axes[i, 3].set_title(\"Augmented Mask\")\n",
    "        axes[i, 3].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = f\"visualizations/{prefix}_epoch_{epoch}_samples.png\"\n",
    "    plt.savefig(save_path)\n",
    "    print(f\" Visualization saved to: {save_path}\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    def denormalize(tensor):\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406], device=tensor.device).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225], device=tensor.device).view(3, 1, 1)\n",
    "        return (tensor * std + mean).clamp(0, 1)\n",
    "\n",
    "    num_samples = min(5, len(original_images))\n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(18, num_samples * 4))\n",
    "\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # --- Original Image ---\n",
    "        axes[i, 0].imshow(original_images[i].permute(1, 2, 0).cpu().numpy())\n",
    "        axes[i, 0].set_title(f\"Original Image\\n{filenames[i]}\")\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # --- Original Mask ---\n",
    "        orig_mask = original_masks[i].cpu().numpy()\n",
    "        if orig_mask.ndim == 3:\n",
    "            orig_mask = orig_mask[0]\n",
    "        axes[i, 1].imshow(orig_mask, cmap='gray')\n",
    "        axes[i, 1].set_title(\"Original Mask\")\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "        # --- Augmented Image ---\n",
    "        aug_img = denormalize(augmented_images[i]).permute(1, 2, 0).cpu().numpy()\n",
    "        aug_img = (aug_img * 255).astype(np.uint8)  # <--- ЭТО ДОБАВИТЬ\n",
    "        axes[i, 2].imshow(aug_img)\n",
    "        axes[i, 2].set_title(f\"Augmented Image\\n{filenames[i]}\")\n",
    "        axes[i, 2].axis('off')\n",
    "\n",
    "        # --- Augmented Mask ---\n",
    "        aug_mask = augmented_masks[i].cpu().numpy()\n",
    "        if aug_mask.ndim == 3:\n",
    "            aug_mask = aug_mask[0]\n",
    "        axes[i, 3].imshow(aug_mask, cmap='gray')\n",
    "        axes[i, 3].set_title(\"Augmented Mask\")\n",
    "        axes[i, 3].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"visualizations/{prefix}_epoch_{epoch}_samples.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def get_train_augmentations():\n",
    "    return A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.Rotate(limit=180, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0, p=0.6),\n",
    "        A.GaussNoise(var_limit=(1.0, 3.0), mean=0.0, p=0.1),\n",
    "        A.MultiplicativeNoise(multiplier=(0.9, 1.1), per_channel=True, p=0.2),\n",
    "        A.GaussianBlur(blur_limit=(3, 5), p=0.3),\n",
    "        A.CoarseDropout(\n",
    "            max_holes=2,\n",
    "            max_height=16,\n",
    "            max_width=16,\n",
    "            min_holes=1,\n",
    "            min_height=8,\n",
    "            min_width=8,\n",
    "            fill_value=0,\n",
    "            p=0.1\n",
    "        ),\n",
    "\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ], additional_targets={'mask': 'mask'})\n",
    "\n",
    "\n",
    "def denormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=tensor.device).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225], device=tensor.device).view(3, 1, 1)\n",
    "    return (tensor * std + mean).clamp(0, 1)\n",
    "\n",
    "\n",
    "class PolypDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.*\")))\n",
    "        self.mask_paths = sorted(glob.glob(os.path.join(mask_dir, \"*.*\")))\n",
    "        assert len(self.image_paths) == len(self.mask_paths), \\\n",
    "            f\"Number of images ({len(self.image_paths)}) and masks ({len(self.mask_paths)}) don't match!\"\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.cvtColor(cv2.imread(self.image_paths[idx]), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "            # Ensure mask is float32 and in [0,1] range\n",
    "            mask = mask.float() / 255.0\n",
    "        else:\n",
    "            image = cv2.resize(image, (256, 256)) / 255.0\n",
    "            mask = (cv2.resize(mask, (256, 256)) > 127).astype(np.float32)  # Changed to float32\n",
    "            image = (image - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "            image = torch.from_numpy(image.transpose(2, 0, 1)).float()\n",
    "            mask = torch.from_numpy(mask).unsqueeze(0).float()  # Ensure float\n",
    "            \n",
    "        filename = os.path.basename(self.image_paths[idx])\n",
    "        return image, mask, filename\n",
    "\n",
    "\n",
    "def analyze_prediction(pred, mask, threshold=0.5):\n",
    "    pred_bin = (pred > threshold).astype(np.uint8)\n",
    "    mask_bin = mask.astype(np.uint8)\n",
    "\n",
    "    total = pred.size\n",
    "    tp = ((pred_bin == 1) & (mask_bin == 1)).sum()\n",
    "    tn = ((pred_bin == 0) & (mask_bin == 0)).sum()\n",
    "    fp = ((pred_bin == 1) & (mask_bin == 0))\n",
    "    fn = ((pred_bin == 0) & (mask_bin == 1))\n",
    "\n",
    "    fp_vals = pred[fp]\n",
    "    fn_vals = pred[fn]\n",
    "\n",
    "    return {\n",
    "        \"tp_percent\": round(tp / total * 100, 2),\n",
    "        \"tn_percent\": round(tn / total * 100, 2),\n",
    "        \"fp_percent\": round(fp.sum() / total * 100, 2),\n",
    "        \"fn_percent\": round(fn.sum() / total * 100, 2),\n",
    "        \"fp_pred_mean\": round(fp_vals.mean() if fp_vals.size > 0 else 0.0, 4),\n",
    "        \"fp_pred_min\": round(fp_vals.min() if fp_vals.size > 0 else 0.0, 4),\n",
    "        \"fp_pred_max\": round(fp_vals.max() if fp_vals.size > 0 else 0.0, 4),\n",
    "        \"fn_pred_mean\": round(fn_vals.mean() if fn_vals.size > 0 else 0.0, 4),\n",
    "        \"fn_pred_min\": round(fn_vals.min() if fn_vals.size > 0 else 0.0, 4),\n",
    "        \"fn_pred_max\": round(fn_vals.max() if fn_vals.size > 0 else 0.0, 4),\n",
    "    }\n",
    "\n",
    "def evaluate_and_save(model, dataloader, criterion, device, save_csv_path, threshold=0.5):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for images, masks, fnames in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "            masks_np = masks.cpu().numpy()\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                pred = probs[i, 0]\n",
    "                gt = masks_np[i, 0]\n",
    "                stats = analyze_prediction(pred, gt, threshold=threshold)\n",
    "                stats.update({\"filename\": fnames[i], \"threshold\": threshold})\n",
    "                results.append(stats)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    os.makedirs(os.path.dirname(save_csv_path), exist_ok=True)\n",
    "    df.to_csv(save_csv_path, index=False)\n",
    "    print(f\"✅ Saved evaluation results to {save_csv_path}\")\n",
    "    return df\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, writer, epoch, dataset_name):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    original_images = []\n",
    "    original_masks = []\n",
    "    augmented_images = []\n",
    "    augmented_masks = []\n",
    "    filenames = []\n",
    "\n",
    "    # Отображение имя файла → путь\n",
    "    path_map = {os.path.basename(p): p for p in dataloader.dataset.image_paths}\n",
    "    mask_map = {os.path.basename(p): p for p in dataloader.dataset.mask_paths}\n",
    "\n",
    "    for batch_idx, (images, masks, fnames) in enumerate(tqdm(dataloader, desc=\"Training\", leave=False)):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        if epoch == 0 and batch_idx == 0:\n",
    "            for i in range(min(5, images.size(0))):\n",
    "                fname = fnames[i]\n",
    "                img_path = path_map[fname]\n",
    "                mask_path = mask_map[fname]\n",
    "\n",
    "                # --- Загружаем оригинал без аугментации и нормализации ---\n",
    "                image_np = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "                mask_np = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "                image_np = cv2.resize(image_np, (256, 256)) / 255.0\n",
    "                mask_np = cv2.resize(mask_np, (256, 256))\n",
    "                mask_np = (mask_np > 127).astype(np.float32)\n",
    "\n",
    "                image_tensor = torch.from_numpy(image_np.transpose(2, 0, 1)).float()\n",
    "                mask_tensor = torch.from_numpy(mask_np).unsqueeze(0)\n",
    "\n",
    "                original_images.append(image_tensor)\n",
    "                original_masks.append(mask_tensor)\n",
    "\n",
    "                # --- Сохраняем то, что реально пошло в модель ---\n",
    "                augmented_images.append(images[i].detach().cpu())\n",
    "                augmented_masks.append(masks[i].detach().cpu())\n",
    "                filenames.append(fname)\n",
    "\n",
    "            visualize_samples_with_names(\n",
    "                original_images,\n",
    "                original_masks,\n",
    "                augmented_images,  # raw input to model\n",
    "                augmented_masks,\n",
    "                filenames,\n",
    "                epoch=epoch,\n",
    "                prefix=\"train\"\n",
    "            )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        preds = (outputs > 0.5).float().detach()\n",
    "        all_y_true.append(masks.cpu().numpy())\n",
    "        all_y_pred.append(preds.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    all_y_true = np.concatenate(all_y_true, axis=0)\n",
    "    all_y_pred = np.concatenate(all_y_pred, axis=0)\n",
    "\n",
    "    iou, recall, precision, f1 = metrics(all_y_true, all_y_pred)\n",
    "\n",
    "    writer.add_scalar(f\"Loss/Train/{dataset_name}\", epoch_loss, epoch)\n",
    "    writer.add_scalar(f\"Metrics/Train_IoU/{dataset_name}\", iou, epoch)\n",
    "    writer.add_scalar(f\"Metrics/Train_Recall/{dataset_name}\", recall, epoch)\n",
    "    writer.add_scalar(f\"Metrics/Train_Precision/{dataset_name}\", precision, epoch)\n",
    "    writer.add_scalar(f\"Metrics/Train_F1/{dataset_name}\", f1, epoch)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return epoch_loss, iou, recall, precision, f1\n",
    "\n",
    "\n",
    "dataset = {\"name\": \"40_60\", \n",
    "     \"train_image_dir\": 'C:/Users/Dara/Desktop/Final_thesis/data_choosing/Images/train/40_60',\n",
    "     \"train_mask_dir\": \"C:/Users/Dara/Desktop/Final_thesis/data_choosing/Masks/train/40_60\",\n",
    "     \"val_image_dir\": \"C:/Users/Dara/Desktop/Final_thesis/data_choosing/Images/val\",\n",
    "     \"val_mask_dir\": \"C:/Users/Dara/Desktop/Final_thesis/data_choosing/Masks/val\",\n",
    "     \"test_image_dir\": \"C:/Users/Dara/Desktop/Final_thesis/data_choosing/Images/test\",\n",
    "     \"test_mask_dir\": \"C:/Users/Dara/Desktop/Final_thesis//data_choosing/Masks/test\"}\n",
    "\n",
    "results = []\n",
    "encoder_name = \"resnet34\"  \n",
    "\n",
    "dataset_name = dataset[\"name\"]\n",
    "print(f\"Training on dataset: {dataset_name}\")\n",
    "\n",
    "train_dataset = PolypDataset(dataset[\"train_image_dir\"], dataset[\"train_mask_dir\"], transform=get_train_augmentations())\n",
    "val_dataset = PolypDataset(dataset[\"val_image_dir\"], dataset[\"val_mask_dir\"])\n",
    "test_dataset = PolypDataset(dataset[\"test_image_dir\"], dataset[\"test_mask_dir\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=encoder_name,\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=1,\n",
    "    activation=\"sigmoid\"\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = DiceBCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "log_dir = f\"logs/{dataset_name}_{encoder_name}\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "patience = 10\n",
    "best_val_iou = 0.0\n",
    "best_epoch = -1\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f\"Эпоха {epoch+1}/{num_epochs} для датасета {dataset_name}\")\n",
    "    \n",
    "#     train_loss, train_iou, train_recall, train_precision, train_f1 = train_one_epoch(\n",
    "#         model, train_loader, criterion, optimizer, device, writer, epoch, dataset_name)\n",
    "#     print(f\"Train Loss: {train_loss:.4f} | IoU: {train_iou:.4f} | Recall: {train_recall:.4f} | Precision: {train_precision:.4f} | F1: {train_f1:.4f}\")\n",
    "    \n",
    "#     val_loss, val_iou, val_recall, val_precision, val_f1 = validate(\n",
    "#         model, val_loader, criterion, device, writer, epoch, dataset_name)\n",
    "#     print(f\"Val Loss: {val_loss:.4f} | IoU: {val_iou:.4f} | Recall: {val_recall:.4f} | Precision: {val_precision:.4f} | F1: {val_f1:.4f}\")\n",
    "\n",
    "#     if val_iou > best_val_iou:\n",
    "#         best_val_iou = val_iou\n",
    "#         best_epoch = epoch\n",
    "#         torch.save(model.state_dict(), f\"unetplusplus_{dataset_name}_best_model.pth\")\n",
    "#         print(\"Новая лучшая модель сохранена.\")\n",
    "#         epochs_without_improvement = 0\n",
    "#     else:\n",
    "#         epochs_without_improvement += 1\n",
    "#         if epochs_without_improvement >= patience:\n",
    "#             print(f\"Ранняя остановка на эпохе {epoch+1}\")\n",
    "#             break\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(f\"unetplusplus_{dataset_name}_best_model.pth\"))\n",
    "test_loss, test_iou, test_recall, test_precision, test_f1 = test_model(\n",
    "    model, test_loader, criterion, device, save_folder=f\"predicted_masks_{dataset_name}\")\n",
    "\n",
    "results.append({\n",
    "    \"Dataset\": dataset_name,\n",
    "    \"Best Epoch\": best_epoch + 1,\n",
    "    \"Test IoU\": test_iou,\n",
    "    \"Test Recall\": test_recall,\n",
    "    \"Test Precision\": test_precision,\n",
    "    \"Test F1\": test_f1,\n",
    "    \"Test Loss\": test_loss\n",
    "})\n",
    "\n",
    "writer.close()\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_with_thresholds(model, dataloader, criterion, device, thresholds=None, base_save_dir=\"threshold_sweep\", dataset_name=\"test\"):\n",
    "\n",
    "    if thresholds is None:\n",
    "        thresholds = [round(x * 0.1, 2) for x in range(1, 10)]  # [0.1, 0.2, ..., 0.9]\n",
    "\n",
    "    summary_metrics = []\n",
    "    summary_path = os.path.join(\"results\", f\"{dataset_name}_threshold_metrics.csv\")\n",
    "\n",
    "    for t in thresholds:\n",
    "        folder = os.path.join(base_save_dir, f\"t{int(t * 100)}\")\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "        # TensorBoard логирование\n",
    "        writer = SummaryWriter(log_dir=os.path.join(\"logs\", f\"{dataset_name}_t{int(t*100)}\"))\n",
    "\n",
    "        # Сохраняем предсказания масок как .png + метрики в CSV\n",
    "        test_loss, test_iou, test_recall, test_precision, test_f1 = test_model(\n",
    "            model, dataloader, criterion, device,\n",
    "            save_folder=folder\n",
    "        )\n",
    "\n",
    "        # Логируем в TensorBoard\n",
    "        writer.add_scalar(f\"Loss/Test/{dataset_name}\", test_loss, t)\n",
    "        writer.add_scalar(f\"Metrics/Test_IoU/{dataset_name}\", test_iou, t)\n",
    "        writer.add_scalar(f\"Metrics/Test_Recall/{dataset_name}\", test_recall, t)\n",
    "        writer.add_scalar(f\"Metrics/Test_Precision/{dataset_name}\", test_precision, t)\n",
    "        writer.add_scalar(f\"Metrics/Test_F1/{dataset_name}\", test_f1, t)\n",
    "        writer.close()\n",
    "\n",
    "        csv_path = os.path.join(\"results\", f\"{dataset_name}_predictions_t{int(t*100)}.csv\")\n",
    "        evaluate_and_save(model, dataloader, criterion, device, save_csv_path=csv_path, threshold=t)\n",
    "\n",
    "        summary_metrics.append({\n",
    "            \"threshold\": t,\n",
    "            \"test_loss\": round(test_loss, 5),\n",
    "            \"mean_iou\": round(test_iou, 5),\n",
    "            \"mean_recall\": round(test_recall, 5),\n",
    "            \"mean_precision\": round(test_precision, 5),\n",
    "            \"mean_f1\": round(test_f1, 5)\n",
    "        })\n",
    "\n",
    "        # Обновляем .csv после каждой итерации\n",
    "        intermediate_df = pd.DataFrame(summary_metrics)\n",
    "        intermediate_df.to_csv(summary_path, index=False)\n",
    "\n",
    "        print(f\" Threshold {t:.2f} — Done. Results saved in: {folder} and {csv_path}\")\n",
    "\n",
    "    print(f\"\\n📊 Summary saved to: {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 193/193 [00:25<00:00,  7.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved evaluation results to results/test_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 193/193 [00:25<00:00,  7.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved evaluation results to results\\40_60_predictions_t10.csv\n",
      "✅ Threshold 0.10 — Done. Results saved in: threshold_sweep\\t10 and results\\40_60_predictions_t10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 193/193 [00:24<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved evaluation results to results/test_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 193/193 [00:24<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved evaluation results to results\\40_60_predictions_t20.csv\n",
      "✅ Threshold 0.20 — Done. Results saved in: threshold_sweep\\t20 and results\\40_60_predictions_t20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 193/193 [00:25<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved evaluation results to results/test_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 193/193 [00:25<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved evaluation results to results\\40_60_predictions_t30.csv\n",
      "✅ Threshold 0.30 — Done. Results saved in: threshold_sweep\\t30 and results\\40_60_predictions_t30.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 193/193 [00:26<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved evaluation results to results/test_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 193/193 [00:25<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved evaluation results to results\\40_60_predictions_t40.csv\n",
      "✅ Threshold 0.40 — Done. Results saved in: threshold_sweep\\t40 and results\\40_60_predictions_t40.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 193/193 [00:25<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved evaluation results to results/test_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 193/193 [00:25<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved evaluation results to results\\40_60_predictions_t50.csv\n",
      "✅ Threshold 0.50 — Done. Results saved in: threshold_sweep\\t50 and results\\40_60_predictions_t50.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 193/193 [00:25<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved evaluation results to results/test_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 193/193 [00:25<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved evaluation results to results\\40_60_predictions_t60.csv\n",
      "✅ Threshold 0.60 — Done. Results saved in: threshold_sweep\\t60 and results\\40_60_predictions_t60.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 193/193 [00:25<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved evaluation results to results/test_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 193/193 [00:24<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved evaluation results to results\\40_60_predictions_t70.csv\n",
      "✅ Threshold 0.70 — Done. Results saved in: threshold_sweep\\t70 and results\\40_60_predictions_t70.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 193/193 [00:25<00:00,  7.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved evaluation results to results/test_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 193/193 [00:24<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved evaluation results to results\\40_60_predictions_t80.csv\n",
      "✅ Threshold 0.80 — Done. Results saved in: threshold_sweep\\t80 and results\\40_60_predictions_t80.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 193/193 [00:25<00:00,  7.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved evaluation results to results/test_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 193/193 [00:24<00:00,  7.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved evaluation results to results\\40_60_predictions_t90.csv\n",
      "✅ Threshold 0.90 — Done. Results saved in: threshold_sweep\\t90 and results\\40_60_predictions_t90.csv\n",
      "\n",
      "📊 Summary saved to: results\\40_60_threshold_metrics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.load_state_dict(torch.load(f\"unetplusplus_{dataset['name']}_best_model.pth\"))\n",
    "model.to(device)\n",
    "\n",
    "test_model_with_thresholds(\n",
    "    model=model,\n",
    "    dataloader=test_loader,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    thresholds=[round(x * 0.1, 2) for x in range(1, 10)],  # 0.1–0.9\n",
    "    base_save_dir=\"threshold_sweep\",\n",
    "    dataset_name=dataset[\"name\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
