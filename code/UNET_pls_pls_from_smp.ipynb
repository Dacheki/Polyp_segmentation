{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dara\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\albumentations\\__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.22). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import jaccard_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import random\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import pandas as pd\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logger = logging.getLogger(\"test_logger\")\n",
    "test_logger.setLevel(logging.INFO)\n",
    "file_handler = logging.FileHandler(\"test_results.log\", mode=\"w\")\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "file_handler.setFormatter(formatter)\n",
    "test_logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 20\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_augmentations():\n",
    "    return A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.Rotate(limit=180, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0, p=0.6),\n",
    "        A.GaussNoise(var_limit=(1.0, 3.0), mean=0.0, p=0.1),\n",
    "        A.MultiplicativeNoise(multiplier=(0.9, 1.1), per_channel=True, p=0.2),\n",
    "        A.GaussianBlur(blur_limit=(3, 5), p=0.3),\n",
    "        A.CoarseDropout(\n",
    "            max_holes=2,\n",
    "            max_height=16,\n",
    "            max_width=16,\n",
    "            min_holes=1,\n",
    "            min_height=8,\n",
    "            min_width=8,\n",
    "            fill_value=0,\n",
    "            p=0.1\n",
    "        ),\n",
    "\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ], additional_targets={'mask': 'mask'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolypDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.*\")))\n",
    "        self.mask_paths = sorted(glob.glob(os.path.join(mask_dir, \"*.*\")))\n",
    "        assert len(self.image_paths) == len(self.mask_paths), \\\n",
    "            f\"Number of images ({len(self.image_paths)}) and masks ({len(self.mask_paths)}) don't match!\"\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.cvtColor(cv2.imread(self.image_paths[idx]), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "            # Ensure mask is float32 and in [0,1] range\n",
    "            mask = mask.float() / 255.0\n",
    "        else:\n",
    "            image = cv2.resize(image, (256, 256)) / 255.0   #Проверь 256!\n",
    "            mask = (cv2.resize(mask, (256, 256)) > 127).astype(np.float32)  # Changed to float32\n",
    "            image = (image - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "            image = torch.from_numpy(image.transpose(2, 0, 1)).float()\n",
    "            mask = torch.from_numpy(mask).unsqueeze(0).float()  # Ensure float\n",
    "            \n",
    "        filename = os.path.basename(self.image_paths[idx])\n",
    "        return image, mask, filename\n",
    "\n",
    "def metrics(y_true, y_pred, y_prob):\n",
    "    # Convert tensors to numpy arrays if needed\n",
    "    if isinstance(y_true, torch.Tensor):\n",
    "        y_true = y_true.cpu().numpy()\n",
    "    if isinstance(y_pred, torch.Tensor):\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "    if isinstance(y_prob, torch.Tensor):  # Исправлено: было y_pred вместо y_prob\n",
    "        y_prob = y_prob.cpu().numpy()\n",
    "\n",
    "    # Prepare binary labels (image-level)\n",
    "    y_true_binar = []\n",
    "    y_pred_binar = []\n",
    "    y_prob_binar = []\n",
    "    \n",
    "    for i in range(len(y_true)):\n",
    "        y_true_binar.append(int(y_true[i].any()))\n",
    "        y_pred_binar.append(int(y_pred[i].any()))\n",
    "        # For probabilities, use max instead of mean for better ROC AUC calculation\n",
    "        y_prob_binar.append(float(y_prob[i].mean()))  # Исправлено: max вместо mean\n",
    "\n",
    "    # Flatten for pixel-level metrics\n",
    "    y_true_flat = y_true.flatten().astype(int)\n",
    "    y_pred_flat = (y_pred.flatten() > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate pixel-level metrics\n",
    "    iou = jaccard_score(y_true_flat, y_pred_flat, zero_division=0)\n",
    "\n",
    "        \n",
    "    dice = (2. * np.sum(y_true_flat * y_pred_flat)) / (np.sum(y_true_flat) + np.sum(y_pred_flat) + 1e-8)\n",
    "    \n",
    "    # Calculate image-level metrics\n",
    "\n",
    "    rec = recall_score(y_true_binar, y_pred_binar, zero_division=0)\n",
    "    prec = precision_score(y_true_binar, y_pred_binar, zero_division=0)\n",
    "    f1 = f1_score(y_true_binar, y_pred_binar, zero_division=0)\n",
    "    auc = roc_auc_score(y_true_binar, y_prob_binar)  # Исправлено: используем y_prob_binar\n",
    "\n",
    "\n",
    "    return iou, rec, prec, f1, auc, dice\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCELoss()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # Ensure target is float (in case it's not)\n",
    "        target = target.float()\n",
    "        \n",
    "        smooth = 1.0\n",
    "        pred = pred.contiguous().view(-1)\n",
    "        target = target.contiguous().view(-1)\n",
    "        \n",
    "        # BCE Loss\n",
    "        bce_loss = self.bce(pred, target)\n",
    "        \n",
    "        # Dice Loss\n",
    "        intersection = (pred * target).sum()\n",
    "        dice_coeff = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "        dice_loss = 1 - dice_coeff\n",
    "        \n",
    "        return dice_loss + bce_loss\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, writer, epoch, dataset_name, treshold):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    all_y_prob = []\n",
    "    for images, masks, _ in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        preds = (outputs > treshold).float().detach()\n",
    "        probs = torch.sigmoid(outputs).detach()\n",
    "        all_y_true.append(masks.cpu().numpy())\n",
    "        all_y_pred.append(preds.cpu().numpy())\n",
    "        all_y_prob.append(probs.cpu().numpy()) \n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    all_y_true = np.concatenate(all_y_true, axis=0)\n",
    "    all_y_pred = np.concatenate(all_y_pred, axis=0)\n",
    "    all_y_prob = np.concatenate(all_y_prob, axis=0)\n",
    "\n",
    "    iou, recall, precision, f1, auc, dice = metrics(all_y_true, all_y_pred, all_y_prob)\n",
    "\n",
    "    writer.add_scalar(f\"Loss/Train/{dataset_name}\", epoch_loss, epoch)\n",
    "    writer.add_scalar(f\"Metrics/Train_IoU/{dataset_name}\", iou, epoch)\n",
    "    writer.add_scalar(f\"Metrics/Train_Recall/{dataset_name}\", recall, epoch)\n",
    "    writer.add_scalar(f\"Metrics/Train_Precision/{dataset_name}\", precision, epoch)\n",
    "    writer.add_scalar(f\"Metrics/Train_F1/{dataset_name}\", f1, epoch)\n",
    "    writer.add_scalar(f\"Metrics/Train_AUC/{dataset_name}\", auc, epoch)\n",
    "    writer.add_scalar(f\"Metrics/Train_DICE/{dataset_name}\", dice, epoch)\n",
    "    torch.cuda.empty_cache()\n",
    "    return epoch_loss, iou, recall, precision, f1, auc, dice\n",
    "\n",
    "def validate(model, dataloader, criterion, device, writer, epoch, dataset_name, treshold):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    all_y_prob = []\n",
    "\n",
    "    for images, masks, _ in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "        probs = torch.sigmoid(outputs).detach()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        preds = (outputs > treshold).float()\n",
    "        all_y_true.append(masks.cpu().numpy())\n",
    "        all_y_pred.append(preds.cpu().numpy())\n",
    "        all_y_prob.append(probs.cpu().numpy()) \n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    all_y_true = np.concatenate(all_y_true, axis=0)\n",
    "    all_y_pred = np.concatenate(all_y_pred, axis=0)\n",
    "    all_y_prob = np.concatenate(all_y_prob, axis=0)\n",
    "\n",
    "\n",
    "    iou, recall, precision, f1, auc, dice = metrics(all_y_true, all_y_pred, all_y_prob)\n",
    "\n",
    "    writer.add_scalar(f\"Loss/Val/{dataset_name}\", epoch_loss, epoch)\n",
    "    writer.add_scalar(f\"Metrics/Val_IoU/{dataset_name}\", iou, epoch)\n",
    "    writer.add_scalar(f\"Metrics/Val_Recall/{dataset_name}\", recall, epoch)\n",
    "    writer.add_scalar(f\"Metrics/Val_Precision/{dataset_name}\", precision, epoch)\n",
    "    writer.add_scalar(f\"Metrics/Val_F1/{dataset_name}\", f1, epoch)\n",
    "    writer.add_scalar(f\"Metrics/Val_AUC/{dataset_name}\", auc, epoch)\n",
    "    writer.add_scalar(f\"Metrics/Val_Train_DICE/{dataset_name}\", dice, epoch)\n",
    "\n",
    "    return epoch_loss, iou, recall, precision, f1, auc, dice\n",
    "\n",
    "def test_model(model, dataloader, criterion, device, save_folder=\".\", treshold = 0.502):\n",
    "    test_logger.info(\"Начало тестирования...\")\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    all_y_prob = []\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks, filenames in tqdm(dataloader, desc=\"Testing\"):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            preds = (outputs > treshold).float()\n",
    "            probs = torch.sigmoid(outputs).detach()\n",
    "\n",
    "            all_y_true.append(masks.cpu().numpy())\n",
    "            all_y_pred.append(preds.cpu().numpy())\n",
    "            all_y_prob.append(probs.cpu().numpy())\n",
    "\n",
    "            preds_np = preds.cpu().numpy()  \n",
    "            for i in range(preds_np.shape[0]):\n",
    "                mask_pred = preds_np[i, 0]\n",
    "                mask_img = (mask_pred * 255).astype(np.uint8)\n",
    "                save_path = os.path.join(save_folder, filenames[i])\n",
    "                cv2.imwrite(save_path, mask_img)\n",
    "\n",
    "    test_loss = running_loss / len(dataloader.dataset)\n",
    "    all_y_true = np.concatenate(all_y_true, axis=0)\n",
    "    all_y_pred = np.concatenate(all_y_pred, axis=0)\n",
    "    all_y_prob = np.concatenate(all_y_prob, axis=0)\n",
    "                                 \n",
    "    iou, recall, precision, f1, auc, dice = metrics(all_y_true, all_y_pred, all_y_prob)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"IoU: {iou:.4f} | Recall: {recall:.4f} | Precision: {precision:.4f} | F1 Score: {f1:.4f} | AUC Score: {auc:.4f} | DICE: {dice:.4f}\")\n",
    "    \n",
    "    test_logger.info(f\"Test Loss: {test_loss:.4f}\")\n",
    "    test_logger.info(f\"IoU: {iou:.4f} | Recall: {recall:.4f} | Precision: {precision:.4f} | F1 Score: {f1:.4f} | AUC Score: {auc:.4f} | DICE: {dice:.4f}\")\n",
    "\n",
    "    return test_loss, iou, recall, precision, f1, auc, dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\"name\": \"40_60\", \n",
    "     \"train_image_dir\": 'C:/Users/Dara/Desktop/Final_thesis/data_choosing/Images/train/40_60',\n",
    "     \"train_mask_dir\": \"C:/Users/Dara/Desktop/Final_thesis/data_choosing/Masks/train/40_60\",\n",
    "     \"val_image_dir\": \"C:/Users/Dara/Desktop/Final_thesis/data_choosing/Images/val\",\n",
    "     \"val_mask_dir\": \"C:/Users/Dara/Desktop/Final_thesis/data_choosing/Masks/val\",\n",
    "     \"test_image_dir\": \"C:/Users/Dara/Desktop/Final_thesis/data_choosing/Images/test\",\n",
    "     \"test_mask_dir\": \"C:/Users/Dara/Desktop/Final_thesis//data_choosing/Masks/test\"}\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on dataset: 40_60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_556\\1662773873.py:11: UserWarning: Argument 'fill_value' is not valid and will be ignored.\n",
      "  A.CoarseDropout(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1/20 для датасета 40_60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/831 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "results = []\n",
    "encoder_name = \"resnet34\"  \n",
    "\n",
    "\n",
    "dataset_name = dataset[\"name\"]\n",
    "print(f\"Training on dataset: {dataset_name}\")\n",
    "\n",
    "train_dataset = PolypDataset(dataset[\"train_image_dir\"], dataset[\"train_mask_dir\"], transform=get_train_augmentations())\n",
    "val_dataset = PolypDataset(dataset[\"val_image_dir\"], dataset[\"val_mask_dir\"])\n",
    "test_dataset = PolypDataset(dataset[\"test_image_dir\"], dataset[\"test_mask_dir\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=encoder_name,\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=1,\n",
    "    activation=\"sigmoid\"\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = DiceBCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "log_dir = f\"logs/{dataset_name}_{encoder_name}\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "patience = 10\n",
    "best_val_dice = 0.0  \n",
    "best_epoch = -1\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Эпоха {epoch+1}/{num_epochs} для датасета {dataset_name}\")\n",
    "    \n",
    "    train_loss, train_iou, train_recall, train_precision, train_f1,train_auc, train_dice = train_one_epoch(\n",
    "        model, train_loader, criterion, optimizer, device, writer, epoch, dataset_name,treshold = 0.502)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | IoU: {train_iou:.4f} | Recall: {train_recall:.4f} | Precision: {train_precision:.4f} | F1: {train_f1:.4f}| AUC Score: {train_auc:.4f} | DICE: {train_dice:.4f}\")\n",
    "    \n",
    "    val_loss, val_iou, val_recall, val_precision, val_f1, val_auc, val_dice= validate(\n",
    "        model, val_loader, criterion, device, writer, epoch, dataset_name,treshold = 0.502)\n",
    "    print(f\"Val Loss: {val_loss:.4f} | IoU: {val_iou:.4f} | Recall: {val_recall:.4f} | Precision: {val_precision:.4f} | F1: {val_f1:.4f} | AUC Score: {val_auc:.4f} | DICE: {val_dice:.4f}\")\n",
    "\n",
    "    if val_dice > best_val_dice:\n",
    "        best_val_dice = val_dice\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), f\"unetplusplus_{dataset_name}_best_model.pth\")\n",
    "        print(\"Новая лучшая модель сохранена.\")\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Ранняя остановка на эпохе {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(f\"Обучение завершено для датасета {dataset_name}. Лучшая модель на эпохе {best_epoch+1} с  DICE: {best_val_dice:.4f}\")\n",
    "\n",
    "model.load_state_dict(torch.load(f\"unetplusplus_{dataset_name}_best_model.pth\"))\n",
    "test_loss, test_iou, test_recall, test_precision, test_f1, test_auc, test_dice = test_model(\n",
    "    model, test_loader, criterion, device, save_folder=f\"predicted_maskss_{dataset_name}\", treshold = 0.502)\n",
    "\n",
    "results.append({\n",
    "    \"Dataset\": dataset_name,\n",
    "    \"Best Epoch\": best_epoch + 1,\n",
    "    \"Test IoU\": test_iou,\n",
    "    \"Test Recall\": test_recall,\n",
    "    \"Test Precision\": test_precision,\n",
    "    \"Test F1\": test_f1,\n",
    "    \"Test Loss\": test_loss,\n",
    "    \"Test AUC\": test_auc,\n",
    "    \"Test DICE\": test_dice\n",
    "\n",
    "})\n",
    "\n",
    "writer.close()\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
